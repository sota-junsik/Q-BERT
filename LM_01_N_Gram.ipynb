{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sota-junsik/Q-BERT/blob/master/LM_01_N_Gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b1835cef4defb3025bd36336d8867ac3ffdee6d2",
        "id": "FpfCp7yD6xax"
      },
      "source": [
        "# Language Models (LMs)\n",
        "\n",
        "- based on the https://medium.com/@philiposbornedata/learning-nlp-language-models-with-real-data-cdff04c51c25\n",
        "- based on [Speech and Language Processing (3rd ed. draft) by Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6ddee5b3d2a749befa951b32e22767d038f7a9b5",
        "id": "I1N0tzjW6xaz"
      },
      "source": [
        "#### Enable equation numbering in jupyter notebook for improved readability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "2b257b8e1fb6232e8c9032f54cd7d54de0411971",
        "id": "76Nv8Tbn6xa0"
      },
      "outputs": [],
      "source": [
        "%%javascript\n",
        "MathJax.Hub.Config({\n",
        "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "424523adaaf5c655bfedc0182b5902eab784ed05",
        "id": "gAmpPzMU6xa0"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "# Part 1\n",
        "---\n",
        "## I. Probabilistic Language Models\n",
        "\n",
        "### Example uses\n",
        "- **Machine Translation:** P(**high** winds tonight) > P(**large** winds tonight)\n",
        "- **Spell Correction:** P(...about fifteen **minutes** from...) > P(...about fifteen **mineuts** from...)\n",
        "- **Speech Recognition:** P(I saw a van) >> P(eyes awe of an)\n",
        "\n",
        "### Aim of Language Models\n",
        "The goal of paobabilistic language modelling is to calculate the probability of a sentence of sequence of words:\n",
        "\n",
        "\\begin{equation}\n",
        "    P(W) = P(w_1,w_2,w_3,...,w_n)\n",
        "\\end{equation}\n",
        "\n",
        "and can be used to find the probability of the next word in the sequence:\n",
        "\n",
        "\\begin{equation}\n",
        "    P(w_5|w_1,w_2,w_3,w_4)\n",
        "\\end{equation}\n",
        "\n",
        "a model that computes either of these is called a **language model (LM)**.\n",
        "\n",
        "---\n",
        "## II. Initial Method for Calcualting Probabilities\n",
        "\n",
        "### Defn: Conditional Probability\n",
        "Let A and B be two events with $P(B) \\neq 0$, the conditional probability of A given B is:\n",
        "\n",
        "\\begin{equation}\n",
        "    P(A|B) = \\frac{P(A,B)}{P(B)}\n",
        "\\end{equation}\n",
        "\n",
        "### Defn: Chain Rule\n",
        "\\begin{equation}\n",
        "    P(x_1,x_2,...,x_n) = P(x_1)P(x_2|x_1)...P(x_n|x_1,...,x_{n-1})\n",
        "\\end{equation}\n",
        "\n",
        "#### The Chain Rule applied to compute the join probability of words in a sentence:\n",
        "\\begin{equation}\n",
        "    P(w_1 w_2 ... w_n) = \\prod_{i} P(w_i|w_1 w_2 ... w_{i-1})\n",
        "\\end{equation}\n",
        "\n",
        "e.g.\n",
        "\n",
        "P(\"its water is so transparent\") =\n",
        "    \n",
        "    P(its)xP(water|its)\n",
        "    x P(is|its water)\n",
        "    x P(so|its water is)\n",
        "    x P(transparent|its water is so)\n",
        "\n",
        "#### Can we estimate this by simply counting and dividing the results by the following?\n",
        "\\begin{equation}\n",
        "    P(transparent|its \\ water \\ is \\ so) = \\frac{count(its \\ water \\ is \\ so \\ transparent)}{count(its \\ water \\ is \\ so)}  \n",
        "\\end{equation}\n",
        "#### NO! Far to many possible sentences that would need to be calculated, we would never have enough data to achieve this.\n",
        "\n",
        "\n",
        "---\n",
        "## III. Methods using the Markov Assumption\n",
        "\n",
        "### Defn: Markov Property\n",
        "\n",
        "*A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. A process with this property is called a Markov process.* [1]\n",
        "\n",
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-09-20%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%209.59.03.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-09-20%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%209.59.03.png)\n",
        "In other words, the probability of the next word can be estimated given only the previous $k$ number of words.\n",
        "\n",
        "e.g.\n",
        "\n",
        "\\begin{equation}\n",
        "    P(transparent|its \\ water \\ is \\ so) \\approx P(transparent|so)\n",
        "\\end{equation}\n",
        "or\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "    P(transparent|its \\ water \\ is \\ so) \\approx P(transparent|is \\ so)\n",
        "\\end{equation}\n",
        "\n",
        "#### General Equation for the Markov Assumption\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "    P(w_i|w_1 w_2 ... w_{i-1}) \\approx P(w_i|w_{i-k} ... w_{i-1})\n",
        "\\end{equation}\n",
        "where k is the number of words in the 'state' to be defined by the user.\n",
        "\n",
        "\n",
        "### Unigram Model (k=1)\n",
        "\n",
        "\\begin{equation}\n",
        "    P(w_1 w_2 ... w_n) \\approx \\prod_{i} P(w_i)\n",
        "\\end{equation}\n",
        "\n",
        "### Bigram Model (k=2)\n",
        "\n",
        "\\begin{equation}\n",
        "    P(w_i|w_1 w_2 ... w_{i-1}) \\approx P(w_i|w_{i-1})\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "---\n",
        "## IV. N-gram Models (k=n)\n",
        "\n",
        "The previous two equations can be extended to compute trigrams, 4-grams, 5-grams, etc. In general, this is an insufficient model of language because **sentences often have long distance dependencies**. For example, the subject of the sentence may be at the start whilst our next word to be predicited occurs more than 10 words later.\n",
        "\n",
        "### Estimating Bigram Probabilities using the Maximum Likelihood Estimate\n",
        "\\begin{equation}\n",
        "    P(w_i|w_{i-1}) = \\frac{count(w_{i-1},w_i)}{count(w_{i-1})}\n",
        "\\end{equation}\n",
        "\n",
        "#### Small Example\n",
        "\n",
        "Three sentences:\n",
        "\n",
        "- < s I am Sam /s >\n",
        "- < s Sam I am /s >\n",
        "- < s I do not like green eggs and ham /s >\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "P(I|<s) = \\frac{count(<s,I)}{count(<s)} = \\frac{2}{3}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "P(am|I) = \\frac{count(I,am)}{count(I)} = \\frac{2}{3}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "[1] https://en.wikipedia.org/wiki/Markov_property\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfxRaf1M6xa1"
      },
      "source": [
        "\n",
        "# Part 2\n",
        "\n",
        "## V. Training and Testing the Language Models (LMs)\n",
        "\n",
        "The corpus used to train our LMs will impact the output predictions. Therefore we need to introduce a methodology for evaluating how well our trained LMs perform. The best trained LM is the one that can correctly **predict the next word of setences in an unseen test set.**\n",
        "\n",
        "This can be time consuming, to build multiple LMs for comparison could take hours to compute. Therefore, we introduce the intrinsic evaluation method of **perplexity**. In short perplexity is a measure of how well a probability distribution or probability model predicts a sample. [3]\n",
        "\n",
        "### Defn: Perplexity\n",
        "\n",
        "Perplexity is the inverse probability of the test set normalised by the number of words, more specifically can be defined by the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "    PP(W) = P(w_1 w_2 ... w_N)^{\\frac{-1}{N}}\n",
        "\\end{equation}\n",
        "\n",
        "e.g. Suppose a sentence consists of random digits [0-9], what is the perplexit of this sentence by a model that asigns an equal probability (i.e. $P = 1/10$) to each digit?\n",
        "\n",
        "\\begin{equation}\n",
        "    PP(W) = (\\frac{1}{10}*\\frac{1}{10}*...*\\frac{1}{10})^{\\frac{-1}{10}} = (\\frac{1}{10}^{10} )^{\\frac{-1}{10}} = \\frac{1}{10}^{-1} = 10\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwPRYxSq6xa1"
      },
      "source": [
        "## VI. Entropy in Information Theory\n",
        "\n",
        "\n",
        "### Prerequisite: 정보량 (I)\n",
        "- 어떤 한 사건(event)에서 기대되는 정보량 (I)을 확률과 관련하여 살펴 봄\n",
        "\n",
        "#### 중요성(significance): 어떤 사건이 일어날 가능성이 작으면 작을수록, 그 사건은 더 많은 정보를 지닌다.\n",
        "\n",
        "중요성 조건은 어떤 사건의 확률이 높을수록 이 사건으로 알려지는 정보량은 적어짐을 나타낸다. 따라서 확률값을 역으로 취하여 중요성에 따른 정보량을 나타낼 수 있다.\n",
        "      \n",
        "$P(x1) > P(x2) ⇒ I(x1) < I(x2)$\n",
        "\n",
        "$I(x) = 1/P(x)$\n",
        "      \n",
        "\n",
        "###       가법성(additivity): 만일 x1, x2 가 독립적인 사건이라면 다음을 만족해야 한다.\n",
        "$I(x1x2) = I(x1) + I(x2)$\n",
        "\n",
        "예)\n",
        "\n",
        "$P(x1)=1/2이면 I(x1)=2$\n",
        "\n",
        "$P(x2)=1/4이면 I(x2)=4$\n",
        "\n",
        "따라서 중요성의 조건이 만족된다. 만일 두 사건이 서로 독립적이라면,\n",
        "\n",
        "$P(x1x2) = P(x1) * P(x2) = 1/2 * 1/4 = 1/8 $\n",
        "\n",
        "$I(x1x2) = 1 /P(x1x2) = 8$\n",
        "\n",
        "그러나 가법성에 따라\n",
        "    \n",
        "$I(x1x2) = I(x1) + I(x2) = 2 + 4 = 6$\n",
        "    \n",
        "이다. 따라서 가법성의 조건이 충족되지 못한다.\n",
        "\n",
        "두 독립 사건의 확률값은 곱으로 이루어지지만, 두 사건의 결합된 정보 내용은 더해져야만 한다. 정보의 가법성을 위해서 곱이 아닌 더하기가 필요하다. 따라서 이와 유사한 기능을 하는 log를 도입하게 된다\n",
        "\n",
        "$log(xy) = log x + log y$\n",
        "\n",
        "즉 확률을 역으로 하여 중요성 기준을 만족시킬 수 있고, log를 사용 하여 가법성의 조건을 만족시킬 수 있다. 이제 이 둘을 결합하면 어떤 확률 변수 x 가 지니는 정보량은 다음과 같이 계산될 수 있다.\n",
        "\n",
        "$I(x) = log1/P(x) = - logP(x)$\n",
        "    \n",
        " 예)\n",
        "\n",
        "$P(x1) = 1/2 이면 I(x1) = −log(1/2) = log2 = 1$\n",
        "\n",
        "$P(x2) = 1/4 이면 I(x2) = −log(1/4) = log4 = 2$\n",
        "    \n",
        "$P(x1) > P(x2) ⇒ I(x1) < I(x2)$ 인 중요성의 조건을 만족한다.\n",
        "\n",
        "$P(x1x2) = P(x1)P(x2) = 1/8 ⇒ I(x1x2) = −log(1/8) = 3$\n",
        "\n",
        "$I(x1x2) = I(x1) + I(x2) = 1 + 2 = 3$\n",
        "    \n",
        "으로 가법성의 조건도 만족하게 된다.\n",
        "\n",
        "      \n",
        "\n",
        "In Information Theory, entropy (denoted $H(X)$) of a random variable X is the expected log probabiltiy:\n",
        "\n",
        "\\begin{equation}\n",
        "    H(X) = - \\sum P(x)log_2 P(x)\n",
        "\\end{equation}\n",
        "\n",
        "and is a measure of uncertainty. [4]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG5BMYah6xa1"
      },
      "source": [
        "### Reason for negative sign:\n",
        "- log(p(x))<0 for all p(x) in (0,1) . p(x) is a probability distribution and therefore the values must range between 0 and 1.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*tee-iLsjN-GRT9WervsaMA.png\" witdth=\"30%\" height=\"20%\" >\n",
        "A plot of log(x). For x values between 0 and 1, log(x) <0 (is negative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a1ff042443614ba53e97d90352dc71dcd4127e67",
        "id": "l9LpXbkM6xa2"
      },
      "source": [
        "\n",
        "#### In other words, entropy is the number of possible states that a system can be.\n",
        "\n",
        "### Entropy of a bias coin toss\n",
        "\n",
        "Say we have the probabilities of heads and tails in a coin toss defined by:\n",
        "\n",
        "- $P(heads) = p$\n",
        "- $P(tails) = 1-p$\n",
        "\n",
        "Then the entropy of this is:\n",
        "\n",
        "\\begin{equation}\n",
        "    H(X) = - \\sum P(x)log_2 P(x) = -[plog_2 p + (1-p)log_2 (1-p)]\n",
        "\\end{equation}\n",
        "\n",
        "If the coin is fair, i.e. p = 0.5, then we have:\n",
        "\n",
        "\\begin{equation}\n",
        "    H(X) = -[0.5log_2 0.5 + (1-0.5)log_2 (1-0.5)] = -[-0.5-0.5] = 1\n",
        "\\end{equation}\n",
        "\n",
        "The full entropy distibution over varying bias probabilities is shown below.\n",
        "\n",
        "\n",
        "[3] https://en.wikipedia.org/wiki/Perplexity\n",
        "[4] https://en.wikipedia.org/wiki/Entropy_(information_theory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l23rhQ7P6xa2"
      },
      "source": [
        "### Basic property 1: Uniform distributions have maximum uncertainty\n",
        "If your goal is to minimize uncertainty, stay away from **uniform probability distributions**.\n",
        "\n",
        "![](https://miro.medium.com/max/1280/1*sYmGmOiSB9XL-rPoqdbnRw.png)\n",
        " uniform distributions have maximum entropy for a given numger of outcomes\n",
        "\n",
        "Here is the plot of the Entropy function as applied to Bernoulli trials (events with two possible outcomes and probabilities p and 1-p):\n",
        "\n",
        "![](https://miro.medium.com/max/600/0*8JwLMUrBgGWM7rWr.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzgq8fjU6xa2"
      },
      "source": [
        "### Basic property 2: Uncertainty is additive for independent events\n",
        "\n",
        "Let A and B be independent events. In other words, knowing the outcome of event A does not tell us anything about the outcome of event B.\n",
        "The uncertainty associated with both events — this is another item on our wish list — should be the sum of the individual uncertainties:\n",
        "\n",
        "![](https://miro.medium.com/max/1252/0*9VDWro34ADgoajyb.png)\n",
        "   uncertainty is additive for independent events\n",
        "   \n",
        "Let’s use the example of flipping two coins to make this more concrete. We can either flip both coins simultaneously or first flip one coin and then flip the other one. Another way to think about this is that we can either report the outcome of the two coin flips at once or separately. The uncertainty is the same in either case.\n",
        "\n",
        "To make this even more concrete, consider two particular coins. The first coin lands heads (H) up with an 80% probability and tails (T) up with a probability of 20%. The probabilities for the other coin are 60% and 40%. If we flip both coins simultaneously, there are four possible outcomes: HH, HT, TH and TT. The corresponding probabilities are given by [ 0.48, 0.32, 0.12, 0.08 ].\n",
        "\n",
        "![](https://miro.medium.com/max/1280/1*5bfcQeKVRypkSt3YBU-BHA.png)\n",
        "The joint entropy (green) for the two independent events is equal to the sum of the individual events (red and blue).\n",
        "\n",
        "Plugging the numbers into the entropy formula, we see that:\n",
        "\n",
        "![](https://miro.medium.com/max/1400/0*1zlK66Cwi8T2q82L.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_uuid": "0a15bc4b84668aa31090ace7a9fd618f8ef86ea3",
        "id": "283tjO0i6xa2",
        "outputId": "4189f0a8-3121-44df-fb69-ea26435529c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log2\n",
            "  \"\"\"\n",
            "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  \"\"\"\n",
            "/home/hpshin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0.5,1,'The Entropy of a Bias Coin as \\n the Probabilitiy of Heads Varies')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "p = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
        "H = [-(p*np.log2(p) + (1-p)*np.log(1-p)) for p in p]\n",
        "# Replace nan output with 0\n",
        "H = [0 if math.isnan(x) else x for x in H]\n",
        "\n",
        "plt.plot(p,H)\n",
        "plt.xlim([-0.05,1.05])\n",
        "plt.ylim([-0.05,1])\n",
        "plt.xlabel('Probability of Heads (p)')\n",
        "plt.ylabel('Entropy (H(p))')\n",
        "plt.title('The Entropy of a Bias Coin as \\n the Probabilitiy of Heads Varies')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgiARqrD6xa3"
      },
      "source": [
        "### Basic property 3: Entropy is measured in bits, if we use log base 2\n",
        "\n",
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202023-03-12%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%2011.51.29.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202023-03-12%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%2011.51.29.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZfgttZH6xa3"
      },
      "source": [
        "---\n",
        "## VII. Entropy of Language\n",
        "\n",
        "### 1. Entropy of a sequence of words:\n",
        "\n",
        "\\begin{equation}\n",
        "    H(w_1 w_2 ... w_n) = - \\sum_{w_1...wn} P(w_1 ... w_n) log_2 P(w_1 ... w_n)\n",
        "\\end{equation}\n",
        "\n",
        "### 2. The per-word entropy rate of a sequence of words\n",
        "\n",
        "\\begin{equation}\n",
        "    \\frac{1}{n} H(w_1 w_2 ... w_n) = \\frac{-1}{n} \\sum_{w1...wn} P(w_1 ... w_n) log_2 P(w_1 ... w_n),2)\n",
        "\\end{equation}\n",
        "\n",
        "### 3. Entropy of a language $L = \\{w_1 ... w_n | 1 < n < \\infty\\}$:\n",
        "\n",
        "\\begin{equation}\n",
        "    H(L) = - \\lim_{n\\to\\infty} \\frac{1}{n} H(w_1 ... w_n)\n",
        "\\end{equation}\n",
        "\n",
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-04-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.08.48.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-04-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.08.48.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "168eaa775ad2c043dcf2c9ce9db6ad0166d70719",
        "id": "jZ_k72ni6xa3"
      },
      "source": [
        "\n",
        "### Defn: Cross Entropy\n",
        "\n",
        "The cross entropy, H(p,m), of a true distribution **p** and a model distribution **m** is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "    H(p,m) = - \\sum_{x} p(x) log_2 m(x)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The lower the cross entropy is the closer it is to the true distribution.\n",
        "\n",
        "### Defn: Cross Entropy of a Sequence of Words\n",
        "\n",
        "\\begin{equation}\n",
        "    H(p,m) = - \\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{w1...wn} p(w_1 ... w_n) log_2 m(w_1 ... w_n)\n",
        "\\end{equation}\n",
        "\n",
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-04-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.03.38.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-04-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.03.38.png)\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2xdH3k26xa3"
      },
      "source": [
        "## VIII. Perplexity and Entropy\n",
        "\n",
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-04-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.12.13.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-04-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.12.13.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcUvoxvN6xa3"
      },
      "source": [
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-04-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.12.54.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-04-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.12.54.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoH0_HjF6xa3"
      },
      "source": [
        "### Cross-Entropy Loss Function\n",
        "\n",
        "- Also called logarithmic loss, log loss or logistic loss. Each predicted class probability is compared to the actual class desired output 0 or 1 and a score/loss is calculated that penalizes the probability based on how far it is from the actual expected value.\n",
        "- The penalty is logarithmic in nature yielding a large score for large differences close to 1 and small score for small differences tending to 0.\n",
        "\n",
        "- Consider a 4-class classification task where an image is classified as either a dog, cat, horse or cheetah.\n",
        "<img src=\"https://miro.medium.com/max/1400/1*KvygqiInUpBzpknb-KVKJw.jpeg\" witdth=\"60%\">\n",
        "\n",
        "- In the above Figure, Softmax converts logits into probabilities.\n",
        "- The purpose of the Cross-Entropy is to take the output probabilities (P) and measure the distance from the truth values (as shown in Figure below).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/882/1*rcvGMOuWLMpnNvJ3Oj7fPA.jpeg\" witdth=\"60%\">\n",
        "\n",
        "- For the example above the desired output is [1,0,0,0] for the class dog but the model outputs [0.775, 0.116, 0.039, 0.070] .\n",
        "\n",
        "- The objective is to make the model output be as close as possible to the desired output (truth values).\n",
        "- During model training, the model weights are iteratively adjusted accordingly with the aim of minimizing the Cross-Entropy loss.\n",
        "- The process of adjusting the weights is what defines model training and as the model keeps training and the loss is getting minimized, we say that the model is learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEUgluUC6xa3"
      },
      "source": [
        "- Cross-entropy loss is used when adjusting model weights during training.\n",
        "- The aim is to minimize the loss, i.e, the smaller the loss the better the model. A perfect model has a cross-entropy loss of 0.\n",
        "- Cross-entropy is defined as\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*1WRlyVw_sQNiPDPYAIXf9A.png\" witdth=\"60%\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUSvIe2Z6xa3"
      },
      "source": [
        "#### Binary Cross-Entropy Loss\n",
        "- For binary classification, we have binary cross-entropy defined as\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*xIB2OxbTmjFtHGE8UTLllw.png\" witdth=\"60%\">\n",
        "- Binary cross-entropy is often calculated as the average cross-entropy across all data examples\n",
        "<img src=\"https://miro.medium.com/max/1400/1*LTGc4T0NKn0b8YUAqzRTMg.png\" witdth=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-Q8BwuK6xa3"
      },
      "source": [
        "#### Example\n",
        "- Consider the classification problem with the following Softmax probabilities (S) and the labels (T).\n",
        "- The objective is to calculate for cross-entropy loss given these information.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/882/1*rcvGMOuWLMpnNvJ3Oj7fPA.jpeg\" witdth=\"60%\">\n",
        "\n",
        "- The categorical cross-entropy is computed as follows\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*Z-pih_yOYXuEYwimEj7HfQ.png\" witdth=\"60%\">\n",
        "\n",
        "- Softmax is continuously differentiable function.\n",
        "- This makes it possible to calculate the derivative of the loss function with respect to every weight in the neural network.\n",
        "- This property allows the model to adjust the weights accordingly to minimize the loss function (model output close to the true values).\n",
        "- Assume that after some iterations of model training the model outputs the following vector of logits\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/994/1*MdS4M50j9Cn9GVdO-tdoXg.png\" witdth=\"60%\">\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*WWibqonxKWdqQrU11ctqqA.png\" witdth=\"60%\">\n",
        "\n",
        "- 0.095 is less than previous loss, that is, 0.3677 implying that the model is learning.\n",
        "- The process of optimization (adjusting weights so that the output is close to true values) continues until training is over.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "61b5b1a57c390967790e34a84995b6f4c3fbb1d9",
        "id": "sBDaOcFP6xa3"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# Part 3\n",
        "---\n",
        "## Challenges in Fitting LMs\n",
        "\n",
        "Due to the output of LMs is dependent on the training corpus, N-grams only work well if the training corpus is similar to the testing dataset and we risk overfitting in training.\n",
        "\n",
        "As with any machine learning method, we would like results that are generalisable to new information.\n",
        "\n",
        "Even harder is how we deal with words that do not even appear in training but are in the test data.\n",
        "\n",
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-09-28%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.33.19.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-09-28%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.33.19.png)\n",
        "\n",
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-09-28%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.32.01.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-09-28%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.32.01.png)\n",
        "\n",
        "## IX. Dealing with Zero Counts in Training: Laplace +1 Smoothing\n",
        "\n",
        "To deal with words that are unseen in training we can introduce add-one smoothing. To do this, we simply add one to the count of each word.\n",
        "\n",
        "This shifts the distribution slightly and is often used in text classification and domains where the number of zeros isn't large. However, this is not often used for n-grams, instead we use more complex methods.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GkLtyXI6xa3"
      },
      "source": [
        "### Adjusted Counting\n",
        "\n",
        "- I added Adjusted count for better understanding\n",
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-09-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%204.43.40.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-09-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%204.43.40.png)\n",
        "\n",
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-09-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%204.43.53.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-09-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%204.43.53.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk3QK7WK6xa3"
      },
      "source": [
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.43.06.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.43.06.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b32TVUlk6xa4"
      },
      "source": [
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.44.17.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.44.17.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMVwUqod6xa4"
      },
      "source": [
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.45.15.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.45.15.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9o4cVb26xa4"
      },
      "source": [
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.45.48.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.45.48.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KZtOhCx6xa4"
      },
      "source": [
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.46.17.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.46.17.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcZr7hVF6xa4"
      },
      "source": [
        "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.47.01.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-04-21%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.47.01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2d0cf0a6c33f6f283d6b047de051ea3a8a3db0d3",
        "id": "VlBFDH2R6xa4"
      },
      "source": [
        "### X. Futher Smoothing Methods\n",
        "\n",
        "Laplace +1 smoothing is used in text classification and domains where the number of zeros isn't large. However, it is not often used for n-grams, some better smothing methods for n-grams are:\n",
        "\n",
        "- Add-k Laplace Smoothing\n",
        "- Good-Turing\n",
        "- Kenser-Ney\n",
        "- Witten-Bell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d26473578fc19bd4c35eae301dd2a45c9a9eb155",
        "id": "7eeHtVHP6xa4"
      },
      "source": [
        "---\n",
        "---\n",
        "# Part 4\n",
        "\n",
        "## Selecting the Language Model to Use\n",
        "\n",
        "We have introduced the first three LMs (unigram, bigram and trigram) but which is best to use?\n",
        "\n",
        "Trigrams are generally provide better outputs than bigrams and bigrams provide better outputs than unigrams but as we increase the complexity the computation time becomes increasingly large. Furthermore, the amount of data available decreases as we increase n (i.e. there will be far fewer next words available in a 10-gram than a bigram model).\n",
        "\n",
        "## XI. Back-off Method: Use trigrams (or higher n model) if there is good evidence to, else use bigrams (or other simpler n-gram model).\n",
        "\n",
        "## XII. Interpolation: Use a mixture of n-gram models\n",
        "\n",
        "### Defn: Simple Interpolation:\n",
        "\n",
        "\\begin{equation}\n",
        "    P(w_3|w_1,w_2) = \\lambda_1 P(w3|w_1,w_2) + \\lambda_2 P(w_3|w_2) + \\lambda_3 P(w_3)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\sum_{i} \\lambda_i = 1$.\n",
        "\n",
        "### Defn: Contidional Context Interpolation:\n",
        "\n",
        "\\begin{equation}\n",
        "    P(w_3|w_1,w_2) = \\lambda_1 (w_{1}^{2})P(w3|w_1,w_2) + \\lambda_2 (w_{1}^{2})P(w_3|w_2) + \\lambda_3 (w_{1}^{2})P(w_3)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "### Calculating $\\lambda$s:\n",
        "\n",
        "Using a held-out subset of the corpus (validation set), find $\\lambda$s that maximise the probability of the held out data:\n",
        "\n",
        "\\begin{equation}\n",
        "    P(w_1,w_2,...,w_n|M(\\lambda_1,\\lambda_2,...,\\lambda_k)) = \\sum_i log P_{M(\\lambda_1,\\lambda_2,...,\\lambda_k)}(w_i|w_{i-1})\n",
        "\\end{equation}\n",
        "\n",
        "Where unknown words are assigned an unknown word token '< Unk >'.\n",
        "\n",
        "\n",
        "### Small Interpolation Example\n",
        "\n",
        "Say we are given the following corpus:\n",
        "\n",
        "- < s I am Sam /s >\n",
        "- < s Sam I am /s >\n",
        "- < s I am Sam /s >\n",
        "- < s I do not like green eggs and Sam /s >\n",
        "\n",
        "Using linear interpolation smoothing with a bigram and unigram model with $\\lambda_1 = \\frac{1}{2}$ and $\\lambda_2 = \\frac{1}{2}$, what is $P(Sam|am)$? (note: include '< s' and '/s >' in calculations)\n",
        "\n",
        "Using the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "    P(w_2|w_1) = lambda_1 P(w_2|w1) + lambda_2 P(w2)\n",
        "\\end{equation}\n",
        "\n",
        "We have in our case:\n",
        "\n",
        "\\begin{equation}\n",
        "    P(Sam|am) = \\frac{1}{2} P(Sam|am) + \\frac{1}{2} P(Sam)\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\n",
        "\\begin{equation}\n",
        "    P(Sam|am) = \\frac{count(am, Sam)}{count(am)} = \\frac{2}{3}\n",
        "\\end{equation}\n",
        "\n",
        "and\n",
        "\n",
        "\\begin{equation}\n",
        "    P(Sam) = \\frac{count(Sam)}{Total \\ num \\ words} = \\frac{4}{25}\n",
        "\\end{equation}\n",
        "\n",
        "Therefore,\n",
        "\n",
        "\\begin{equation}\n",
        "    P(Sam|am) = \\frac{1}{2}*\\frac{2}{3} + \\frac{1}{2}*\\frac{4}{25} \\approx 0.413\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnWYgPS66xa4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}